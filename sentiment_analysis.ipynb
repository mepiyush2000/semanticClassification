{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11536</th>\n",
       "      <td>14633</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir my flight was Cancelled Flightled...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11537</th>\n",
       "      <td>14634</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir right on cue with the delaysðŸ‘Œ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11538</th>\n",
       "      <td>14635</td>\n",
       "      <td>positive</td>\n",
       "      <td>@AmericanAir thank you we got on a different f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11539</th>\n",
       "      <td>14636</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11540</th>\n",
       "      <td>14638</td>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir you have my money, you change my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11541 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0 airline_sentiment  \\\n",
       "0               1          positive   \n",
       "1               3          negative   \n",
       "2               4          negative   \n",
       "3               5          negative   \n",
       "4               6          positive   \n",
       "...           ...               ...   \n",
       "11536       14633          negative   \n",
       "11537       14634          negative   \n",
       "11538       14635          positive   \n",
       "11539       14636          negative   \n",
       "11540       14638          negative   \n",
       "\n",
       "                                                    text  \n",
       "0      @VirginAmerica plus you've added commercials t...  \n",
       "1      @VirginAmerica it's really aggressive to blast...  \n",
       "2      @VirginAmerica and it's a really big bad thing...  \n",
       "3      @VirginAmerica seriously would pay $30 a fligh...  \n",
       "4      @VirginAmerica yes, nearly every time I fly VX...  \n",
       "...                                                  ...  \n",
       "11536  @AmericanAir my flight was Cancelled Flightled...  \n",
       "11537         @AmericanAir right on cue with the delaysðŸ‘Œ  \n",
       "11538  @AmericanAir thank you we got on a different f...  \n",
       "11539  @AmericanAir leaving over 20 minutes Late Flig...  \n",
       "11540  @AmericanAir you have my money, you change my ...  \n",
       "\n",
       "[11541 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"airline_sentiment_analysis.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import regex\n",
    "import emoji\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])  \n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.preprocessing.text import Tokenizer, tokenizer_from_json\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "data = pd.read_csv(\"airline_sentiment_analysis.csv\")\n",
    "data\n",
    "\n",
    "class SentimentPrediction():\n",
    "    \n",
    "    def __init__ (self, do_training=True, model = None, json_str_tokenizer = None):\n",
    "\n",
    "        if(model): self.model = model\n",
    "        if(json_str_tokenizer): self.tokenizer = json_str_tokenizer\n",
    "        self.do_training = do_training\n",
    "        \n",
    "        \n",
    "    def preprocess_data(self, text_array, label_array=None):\n",
    "        \n",
    "        #remove emojis with relevant words\n",
    "        data = pd.DataFrame()\n",
    "        data[\"text\"] = text_array\n",
    "        data[\"label\"] = label_array\n",
    "       \n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x:emoji.demojize(x, delimiters=(\" \", \" \")))\n",
    "\n",
    "        #remove tags & urls\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x: regex.sub(r'@\\w+\\S', '', x))\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x: regex.sub(r'http\\S+', '', x))\n",
    "\n",
    "        # Make text lowercase\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x:x.lower())\n",
    "\n",
    "        # Remove punctuation\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda x:x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "        #Remove stopwords\n",
    "        stop_words = stopwords.words('english')\n",
    "        data['text'] = data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop_words))\n",
    "\n",
    "        #lemmatizing\n",
    "        data[\"text\"] = data[\"text\"].apply(lambda text:\" \".join(token.lemma_ for token in nlp(text)))\n",
    "        \n",
    "        data_X = np.array(data[\"text\"])\n",
    "        \n",
    "        if(self.do_training): \n",
    "            data_Y = np.array(data[\"label\"].map({\"positive\":1, \"negative\":0}))   \n",
    "        else: \n",
    "            data_Y = None\n",
    "            \n",
    "        return data_X, data_Y\n",
    "    \n",
    "    \n",
    "    def train(\n",
    "        self, text_array, label_array, \n",
    "          max_sent_len = 100, \n",
    "          max_words = 1000, \n",
    "          batch_size=16, \n",
    "          epochs=10, \n",
    "          val_split=0.2\n",
    "         ):\n",
    "        \n",
    "        if(not self.do_training): \n",
    "            (print(\"do_training parameter is set to False. Set it to True\"))\n",
    "            return\n",
    "        \n",
    "        xtrain, ytrain = self.preprocess_data(text_array, label_array)\n",
    "        print(\"Preprocessing Done\")\n",
    "        \n",
    "        tokenizer = Tokenizer(num_words=max_words)\n",
    "        tokenizer.fit_on_texts(list(xtrain))\n",
    "        sequences = tokenizer.texts_to_sequences(list(xtrain))\n",
    "        tokenized_data = pad_sequences(sequences, maxlen=max_sent_len)\n",
    "\n",
    "        model = Sequential([\n",
    "            layers.Embedding(max_words, 40, input_length=max_sent_len),\n",
    "            layers.Bidirectional(layers.LSTM(20,dropout=0.5)),\n",
    "            layers.Dense(1,activation='sigmoid'),\n",
    "\n",
    "        ])\n",
    "\n",
    "        METRICS = [\n",
    "          keras.metrics.Precision(name='precision'),\n",
    "          keras.metrics.Recall(name='recall'),\n",
    "          keras.metrics.AUC(name='auc'),\n",
    "        ]\n",
    "\n",
    "        model.compile(\n",
    "          optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
    "          loss=keras.losses.BinaryCrossentropy(),\n",
    "          metrics=METRICS)\n",
    "        print(\"MODEL_SUMMARY\")\n",
    "        print(model.summary())\n",
    "        \n",
    "        model.fit(\n",
    "            np.array(tokenized_data), np.array(ytrain),\n",
    "            batch_size = batch_size, #16,\n",
    "            epochs = epochs, #10,\n",
    "            validation_split = val_split, #0.2\n",
    "        )\n",
    "        \n",
    "        return model, tokenizer.to_json()\n",
    "    \n",
    "    \n",
    "    def predict(self, text_array, model_= None, tokenizer_json= None):\n",
    "        \n",
    "        test_str, _ = self.preprocess_data(text_array)\n",
    "        \n",
    "        \n",
    "        if(not (model_ and tokenizer_json)):\n",
    "            model_ = self.model\n",
    "            tokenizer_json = self.tokenizer\n",
    "            \n",
    "        tokenizer_ = tokenizer_from_json(tokenizer_json)\n",
    "        sequences = tokenizer_.texts_to_sequences(list(test_str))\n",
    "        tkd = pad_sequences(sequences, maxlen=100)\n",
    "        pr = model_.predict(tkd) > 0.5\n",
    "        if(pr[0]==0): return \"negative\"\n",
    "        else: return \"positive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Done\n",
      "MODEL_SUMMARY\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 100, 40)           40000     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 40)               9760      \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 41        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 49,801\n",
      "Trainable params: 49,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "577/577 [==============================] - 25s 37ms/step - loss: 0.3624 - precision: 0.7935 - recall: 0.4114 - auc: 0.8600 - val_loss: 0.2013 - val_precision: 0.7314 - val_recall: 0.7619 - val_auc: 0.9399\n",
      "Epoch 2/10\n",
      "577/577 [==============================] - 22s 39ms/step - loss: 0.2238 - precision: 0.8322 - recall: 0.7390 - auc: 0.9501 - val_loss: 0.1808 - val_precision: 0.7883 - val_recall: 0.7202 - val_auc: 0.9456\n",
      "Epoch 3/10\n",
      "577/577 [==============================] - 25s 44ms/step - loss: 0.1981 - precision: 0.8384 - recall: 0.7755 - auc: 0.9617 - val_loss: 0.1856 - val_precision: 0.7380 - val_recall: 0.7292 - val_auc: 0.9458\n",
      "Epoch 4/10\n",
      "577/577 [==============================] - 28s 49ms/step - loss: 0.1849 - precision: 0.8553 - recall: 0.7963 - auc: 0.9667 - val_loss: 0.1846 - val_precision: 0.7291 - val_recall: 0.7530 - val_auc: 0.9509\n",
      "Epoch 5/10\n",
      "577/577 [==============================] - 26s 45ms/step - loss: 0.1733 - precision: 0.8640 - recall: 0.8120 - auc: 0.9706 - val_loss: 0.1883 - val_precision: 0.7719 - val_recall: 0.7351 - val_auc: 0.9485\n",
      "Epoch 6/10\n",
      "577/577 [==============================] - 27s 47ms/step - loss: 0.1650 - precision: 0.8671 - recall: 0.8239 - auc: 0.9740 - val_loss: 0.2003 - val_precision: 0.7818 - val_recall: 0.7143 - val_auc: 0.9441\n",
      "Epoch 7/10\n",
      "577/577 [==============================] - 26s 44ms/step - loss: 0.1560 - precision: 0.8813 - recall: 0.8278 - auc: 0.9767 - val_loss: 0.2001 - val_precision: 0.7643 - val_recall: 0.7143 - val_auc: 0.9427\n",
      "Epoch 8/10\n",
      "577/577 [==============================] - 25s 44ms/step - loss: 0.1518 - precision: 0.8872 - recall: 0.8303 - auc: 0.9776 - val_loss: 0.2207 - val_precision: 0.7109 - val_recall: 0.7173 - val_auc: 0.9419\n",
      "Epoch 9/10\n",
      "577/577 [==============================] - 29s 50ms/step - loss: 0.1425 - precision: 0.8878 - recall: 0.8392 - auc: 0.9812 - val_loss: 0.2167 - val_precision: 0.7159 - val_recall: 0.7351 - val_auc: 0.9420\n",
      "Epoch 10/10\n",
      "577/577 [==============================] - 26s 46ms/step - loss: 0.1410 - precision: 0.8798 - recall: 0.8377 - auc: 0.9821 - val_loss: 0.2164 - val_precision: 0.7106 - val_recall: 0.7381 - val_auc: 0.9416\n"
     ]
    }
   ],
   "source": [
    "dgh = SentimentPrediction()\n",
    "model, tokenizer_ = dgh.train(list(data[\"text\"]), list(data[\"airline_sentiment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'positive'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dgh.predict([\"hi\"], model, tokenizer_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post('/predict')\n",
    "async def predict_species():\n",
    "\n",
    "    dgh = SentimentPrediction()\n",
    "    pred = dgh.predict([\"hi\"], model, tokenizer_)\n",
    "    return {\n",
    "        'prediction': pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
